{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64, requests, schedule, time, json, pytz, logging, os\n",
    "from requests.exceptions import ConnectionError\n",
    "from datetime import datetime, timedelta\n",
    "# from influxdb import InfluxDBClient\n",
    "# from influxdb.exceptions import InfluxDBClientError\n",
    "from influxdb_client import InfluxDBClient, Point\n",
    "from influxdb_client.client.exceptions import InfluxDBError\n",
    "from influxdb_client.client.write_api import SYNCHRONOUS\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: FITBIT_LOG_FILE_PATH=fitbit_log.log\n",
      "env: TOKEN_FILE_PATH=token.json\n",
      "env: CLIENT_ID=23RJP4\n",
      "env: CLIENT_SECRET=79ec7de0c86b40738cadb8ef6c4c486a\n",
      "env: DEVICENAME=Ying Charge 5\n",
      "env: REFRESH_TOKEN=905dcae6869324a1a2aec545c9d2ebae44b7f6d6dbb837feb2ac8812128f9e23\n",
      "env: INFLUXDB_V2_URL=http://10.10.194.5:8086\n",
      "env: INFLUXDB_V2_ORG=walzen\n",
      "env: INFLUXDB_V2_TOKEN=6VnyutFGRy7J1-CAOQoyORzmP-fmGSpRfuSETqReS9oBMocmu6ZYpSSVFHtqYev95sBvP5a9U9i8fPSUbkM0oA==\n",
      "env: INFLUXDB_V2_BUCKET=ying_fitbit\n"
     ]
    }
   ],
   "source": [
    "%env FITBIT_LOG_FILE_PATH=fitbit_log.log\n",
    "%env TOKEN_FILE_PATH=token.json\n",
    "%env CLIENT_ID=23RJP4\n",
    "%env CLIENT_SECRET=79ec7de0c86b40738cadb8ef6c4c486a\n",
    "%env DEVICENAME=Ying Charge 5\n",
    "%env REFRESH_TOKEN=905dcae6869324a1a2aec545c9d2ebae44b7f6d6dbb837feb2ac8812128f9e23\n",
    "%env INFLUXDB_V2_URL=http://10.10.194.5:8086\n",
    "%env INFLUXDB_V2_ORG=walzen\n",
    "%env INFLUXDB_V2_TOKEN=6VnyutFGRy7J1-CAOQoyORzmP-fmGSpRfuSETqReS9oBMocmu6ZYpSSVFHtqYev95sBvP5a9U9i8fPSUbkM0oA==\n",
    "%env INFLUXDB_V2_BUCKET=ying_fitbit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "FITBIT_LOG_FILE_PATH = os.environ.get(\"FITBIT_LOG_FILE_PATH\") or \"your/expected/log/file/location/path\"\n",
    "TOKEN_FILE_PATH = os.environ.get(\"TOKEN_FILE_PATH\") or \"your/expected/token/file/location/path\"\n",
    "OVERWRITE_LOG_FILE = True\n",
    "FITBIT_LANGUAGE = 'en_US'\n",
    "INFLUXDB_BUCKET = os.environ.get(\"INFLUXDB_BUCKET\") or 'your_influxdb_bucket_name'\n",
    "# MAKE SURE you set the application type to PERSONAL. Otherwise, you won't have access to intraday data series, resulting in 40X errors.\n",
    "client_id = os.environ.get(\"CLIENT_ID\") or \"your_application_client_ID\" # Change this to your client ID\n",
    "client_secret = os.environ.get(\"CLIENT_SECRET\") or \"your_application_client_secret\" # Change this to your client Secret\n",
    "DEVICENAME = os.environ.get(\"DEVICENAME\") or \"Your_Device_Name\" # e.g. \"Charge5\"\n",
    "ACCESS_TOKEN = \"\" # Empty Global variable initialization, will be replaced with a functional access code later using the refresh code\n",
    "AUTO_DATE_RANGE = True # Automatically selects date range from todays date and update_date_range variable\n",
    "auto_update_date_range = 1 # Days to go back from today for AUTO_DATE_RANGE *** DO NOT go above 2 - otherwise may break rate limit ***\n",
    "LOCAL_TIMEZONE = \"Automatic\" # set to \"Automatic\" for Automatic setup from User profile (if not mentioned here specifically).\n",
    "SCHEDULE_AUTO_UPDATE = True if AUTO_DATE_RANGE else False # Scheduling updates of data when script runs\n",
    "SERVER_ERROR_MAX_RETRY = 3\n",
    "EXPIRED_TOKEN_MAX_RETRY = 5\n",
    "SKIP_REQUEST_ON_SERVER_ERROR = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OVERWRITE_LOG_FILE:\n",
    "    with open(FITBIT_LOG_FILE_PATH, \"w\"): pass\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    filename=FITBIT_LOG_FILE_PATH,\n",
    "    filemode=\"a\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up base API Caller function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic Request caller for all \n",
    "def request_data_from_fitbit(url, headers={}, params={}, data={}, request_type=\"get\"):\n",
    "    global ACCESS_TOKEN\n",
    "    retry_attempts = 0\n",
    "    logging.debug(\"Requesting data from fitbit via Url : \" + url)\n",
    "    while True: # Unlimited Retry attempts\n",
    "        if request_type == \"get\":\n",
    "            headers = {\n",
    "                \"Authorization\": f\"Bearer {ACCESS_TOKEN}\",\n",
    "                \"Accept\": \"application/json\",\n",
    "                'Accept-Language': FITBIT_LANGUAGE\n",
    "            }\n",
    "        try:        \n",
    "            if request_type == \"get\":\n",
    "                response = requests.get(url, headers=headers, params=params, data=data)\n",
    "            elif request_type == \"post\":\n",
    "                response = requests.post(url, headers=headers, params=params, data=data)\n",
    "            else:\n",
    "                raise Exception(\"Invalid request type \" + str(request_type))\n",
    "        \n",
    "            if response.status_code == 200: # Success\n",
    "                return response.json()\n",
    "            elif response.status_code == 429: # API Limit reached\n",
    "                retry_after = int(response.headers[\"Retry-After\"]) + 300\n",
    "                logging.warning(\"Fitbit API limit reached. Error code : \" + str(response.status_code) + \", Retrying in \" + str(retry_after) + \" seconds\")\n",
    "                print(\"Fitbit API limit reached. Error code : \" + str(response.status_code) + \", Retrying in \" + str(retry_after) + \" seconds\")\n",
    "                time.sleep(retry_after)\n",
    "            elif response.status_code == 401: # Access token expired ( most likely )\n",
    "                logging.info(\"Current Access Token : \" + ACCESS_TOKEN)\n",
    "                logging.warning(\"Error code : \" + str(response.status_code) + \", Details : \" + response.text)\n",
    "                print(\"Error code : \" + str(response.status_code) + \", Details : \" + response.text)\n",
    "                ACCESS_TOKEN = Get_New_Access_Token(client_id, client_secret)\n",
    "                logging.info(\"New Access Token : \" + ACCESS_TOKEN)\n",
    "                time.sleep(30)\n",
    "                if retry_attempts > EXPIRED_TOKEN_MAX_RETRY:\n",
    "                    logging.error(\"Unable to solve the 401 Error. Please debug - \" + response.text)\n",
    "                    raise Exception(\"Unable to solve the 401 Error. Please debug - \" + response.text)\n",
    "            elif response.status_code in [500, 502, 503, 504]: # Fitbit server is down or not responding ( most likely ):\n",
    "                logging.warning(\"Server Error encountered ( Code 5xx ): Retrying after 120 seconds....\")\n",
    "                time.sleep(120)\n",
    "                if retry_attempts > SERVER_ERROR_MAX_RETRY:\n",
    "                    logging.error(\"Unable to solve the server Error. Retry limit exceed. Please debug - \" + response.text)\n",
    "                    if SKIP_REQUEST_ON_SERVER_ERROR:\n",
    "                        logging.warning(\"Retry limit reached for server error : Skipping request -> \" + url)\n",
    "                        return None\n",
    "            else:\n",
    "                logging.error(\"Fitbit API request failed. Status code: \" + str(response.status_code) + \" \" + str(response.text) )\n",
    "                print(f\"Fitbit API request failed. Status code: {response.status_code}\", response.text)\n",
    "                response.raise_for_status()\n",
    "                return None\n",
    "\n",
    "        except ConnectionError as e:\n",
    "            logging.error(\"Retrying in 5 minutes - Failed to connect to internet : \" + str(e))\n",
    "            print(\"Retrying in 5 minutes - Failed to connect to internet : \" + str(e))\n",
    "        retry_attempts += 1\n",
    "        time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Refresh Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refresh_fitbit_tokens(client_id, client_secret, refresh_token):\n",
    "    logging.info(\"Attempting to refresh tokens...\")\n",
    "    url = \"https://api.fitbit.com/oauth2/token\"\n",
    "    headers = {\n",
    "        \"Authorization\": \"Basic \" + base64.b64encode((client_id + \":\" + client_secret).encode()).decode(),\n",
    "        \"Content-Type\": \"application/x-www-form-urlencoded\"\n",
    "    }\n",
    "    data = {\n",
    "        \"grant_type\": \"refresh_token\",\n",
    "        \"refresh_token\": refresh_token\n",
    "    }\n",
    "    json_data = request_data_from_fitbit(url, headers=headers, data=data, request_type=\"post\")\n",
    "    access_token = json_data[\"access_token\"]\n",
    "    new_refresh_token = json_data[\"refresh_token\"]\n",
    "    tokens = {\n",
    "        \"access_token\": access_token,\n",
    "        \"refresh_token\": new_refresh_token\n",
    "    }\n",
    "    with open(TOKEN_FILE_PATH, \"w\") as file:\n",
    "        json.dump(tokens, file)\n",
    "    logging.info(\"Fitbit token refresh successful!\")\n",
    "    return access_token, new_refresh_token\n",
    "\n",
    "def load_tokens_from_file():\n",
    "    with open(TOKEN_FILE_PATH, \"r\") as file:\n",
    "        tokens = json.load(file)\n",
    "        return tokens.get(\"access_token\"), tokens.get(\"refresh_token\")\n",
    "\n",
    "def Get_New_Access_Token(client_id, client_secret):\n",
    "    try:\n",
    "        access_token, refresh_token = load_tokens_from_file()\n",
    "    except FileNotFoundError:\n",
    "        if os.environ.get(\"REFRESH_TOKEN\") is None:\n",
    "            refresh_token = input(\"No token file found. Please enter a valid refresh token : \")\n",
    "        else:\n",
    "            refresh_token = os.environ.get(\"REFRESH_TOKEN\")\n",
    "            os.environ.pop(\"REFRESH_TOKEN\")\n",
    "    access_token, refresh_token = refresh_fitbit_tokens(client_id, client_secret, refresh_token)\n",
    "    return access_token\n",
    "\n",
    "ACCESS_TOKEN = Get_New_Access_Token(client_id, client_secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Influxdb Database Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected successfully!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    influxdbclient = InfluxDBClient.from_env_properties()\n",
    "    write_api = influxdbclient.write_api(write_options=SYNCHRONOUS)\n",
    "except InfluxDBError as e:\n",
    "    logging.error(\"InfluxDB Error : \" + str(e))\n",
    "\n",
    "health = influxdbclient.health()\n",
    "\n",
    "if health.status == \"pass\":\n",
    "    print(\"Connected successfully!\")\n",
    "else:\n",
    "    print(\"Failed to connect!\")\n",
    "\n",
    "def write_points_to_influxdb(points: list[Point]):\n",
    "    logging.info(f\"Updating InfluxDB database with {len(points)} points\")\n",
    "    try:\n",
    "        # write in chunks of 1000 points\n",
    "        for i in range(0, len(points), 1000):\n",
    "            write_api.write(bucket=INFLUXDB_BUCKET, org=os.environ.get(\"INFLUXDB_V2_ORG\"), record=points[i:i+1000])\n",
    "        logging.info(\"Successfully updated InfluxDB database with new points\")\n",
    "    except InfluxDBError as e:\n",
    "        logging.error(\"InfluxDB Error : \" + str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Dates for update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "if AUTO_DATE_RANGE:\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=auto_update_date_range)\n",
    "    end_date_str = end_date.strftime(\"%Y-%m-%d\")\n",
    "    start_date_str = start_date.strftime(\"%Y-%m-%d\")\n",
    "else:\n",
    "    start_date_str = input(\"Enter start date in YYYY-MM-DD format : \")\n",
    "    end_date_str = input(\"Enter end date in YYYY-MM-DD format : \")\n",
    "    start_date = datetime.strptime(start_date_str, \"%Y-%m-%d\")\n",
    "    end_date = datetime.strptime(end_date_str, \"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up functions for Requesting data from server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_records = []\n",
    "\n",
    "def update_working_dates():\n",
    "    global end_date, start_date, end_date_str, start_date_str\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=auto_update_date_range)\n",
    "    end_date_str = end_date.strftime(\"%Y-%m-%d\")\n",
    "    start_date_str = start_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Get last synced battery level of the device\n",
    "def get_battery_level():\n",
    "    device = request_data_from_fitbit(\"https://api.fitbit.com/1/user/-/devices.json\")[0]\n",
    "    if device != None:\n",
    "        dictionary = {\n",
    "            \"measurement\": \"DeviceBatteryLevel\",\n",
    "            \"time\": LOCAL_TIMEZONE.localize(datetime.fromisoformat(device['lastSyncTime'])).astimezone(pytz.utc).isoformat(),\n",
    "            \"fields\": {\n",
    "                \"value\": float(device['batteryLevel'])\n",
    "            }\n",
    "        }\n",
    "        collected_records.append(Point.from_dict(dictionary))\n",
    "        logging.info(\"Recorded battery level for \" + DEVICENAME)\n",
    "    else:\n",
    "        logging.error(\"Recording battery level failed : \" + DEVICENAME)\n",
    "\n",
    "# For intraday detailed data, max possible range in one day. \n",
    "def get_intraday_data_limit_1d(date_str, measurement_list):\n",
    "    for measurement in measurement_list:\n",
    "        data = request_data_from_fitbit('https://api.fitbit.com/1/user/-/activities/' + measurement[0] + '/date/' + date_str + '/1d/' + measurement[2] + '.json')[\"activities-\" + measurement[0] + \"-intraday\"]['dataset']\n",
    "        if data != None:\n",
    "            for value in data:\n",
    "                log_time = datetime.fromisoformat(date_str + \"T\" + value['time'])\n",
    "                utc_time = LOCAL_TIMEZONE.localize(log_time).astimezone(pytz.utc).isoformat()\n",
    "                dictionary = {\n",
    "                        \"measurement\":  measurement[1],\n",
    "                        \"time\": utc_time,\n",
    "                        \"tags\": {\n",
    "                            \"Device\": DEVICENAME\n",
    "                        },\n",
    "                        \"fields\": {\n",
    "                            \"value\": int(value['value'])\n",
    "                        }\n",
    "                    }\n",
    "                collected_records.append(Point.from_dict(dictionary))\n",
    "            logging.info(\"Recorded \" +  measurement[1] + \" intraday for date \" + date_str)\n",
    "        else:\n",
    "            logging.error(\"Recording failed : \" +  measurement[1] + \" intraday for date \" + date_str)\n",
    "\n",
    "# Max range is 30 days, records BR, SPO2 Intraday, skin temp and HRV - 4 queries\n",
    "def get_daily_data_limit_30d(start_date_str, end_date_str):\n",
    "\n",
    "    hrv_data_list = request_data_from_fitbit('https://api.fitbit.com/1/user/-/hrv/date/' + start_date_str + '/' + end_date_str + '.json')['hrv']\n",
    "    if hrv_data_list != None:\n",
    "        for data in hrv_data_list:\n",
    "            log_time = datetime.fromisoformat(data[\"dateTime\"] + \"T\" + \"00:00:00\")\n",
    "            utc_time = LOCAL_TIMEZONE.localize(log_time).astimezone(pytz.utc).isoformat()\n",
    "            dictionary = {\n",
    "                    \"measurement\":  \"HRV\",\n",
    "                    \"time\": utc_time,\n",
    "                    \"tags\": {\n",
    "                        \"Device\": DEVICENAME\n",
    "                    },\n",
    "                    \"fields\": {\n",
    "                        \"dailyRmssd\": data[\"value\"][\"dailyRmssd\"],\n",
    "                        \"deepRmssd\": data[\"value\"][\"deepRmssd\"]\n",
    "                    }\n",
    "                }\n",
    "            collected_records.append(Point.from_dict(dictionary))\n",
    "        logging.info(\"Recorded HRV for date \" + start_date_str + \" to \" + end_date_str)\n",
    "    else:\n",
    "        logging.error(\"Recording failed HRV for date \" + start_date_str + \" to \" + end_date_str)\n",
    "\n",
    "    br_data_list = request_data_from_fitbit('https://api.fitbit.com/1/user/-/br/date/' + start_date_str + '/' + end_date_str + '.json')[\"br\"]\n",
    "    if br_data_list != None:\n",
    "        for data in br_data_list:\n",
    "            log_time = datetime.fromisoformat(data[\"dateTime\"] + \"T\" + \"00:00:00\")\n",
    "            utc_time = LOCAL_TIMEZONE.localize(log_time).astimezone(pytz.utc).isoformat()\n",
    "            dictionary = {\n",
    "                    \"measurement\":  \"BreathingRate\",\n",
    "                    \"time\": utc_time,\n",
    "                    \"tags\": {\n",
    "                        \"Device\": DEVICENAME\n",
    "                    },\n",
    "                    \"fields\": {\n",
    "                        \"value\": data[\"value\"][\"breathingRate\"]\n",
    "                    }\n",
    "                }\n",
    "            collected_records.append(Point.from_dict(dictionary))\n",
    "        logging.info(\"Recorded BR for date \" + start_date_str + \" to \" + end_date_str)\n",
    "    else:\n",
    "        logging.error(\"Recording failed : BR for date \" + start_date_str + \" to \" + end_date_str)\n",
    "\n",
    "    skin_temp_data_list = request_data_from_fitbit('https://api.fitbit.com/1/user/-/temp/skin/date/' + start_date_str + '/' + end_date_str + '.json')[\"tempSkin\"]\n",
    "    if skin_temp_data_list != None:\n",
    "        for temp_record in skin_temp_data_list:\n",
    "            log_time = datetime.fromisoformat(temp_record[\"dateTime\"] + \"T\" + \"00:00:00\")\n",
    "            utc_time = LOCAL_TIMEZONE.localize(log_time).astimezone(pytz.utc).isoformat()\n",
    "            dictionary = {\n",
    "                    \"measurement\":  \"Skin Temperature Variation\",\n",
    "                    \"time\": utc_time,\n",
    "                    \"tags\": {\n",
    "                        \"Device\": DEVICENAME\n",
    "                    },\n",
    "                    \"fields\": {\n",
    "                        \"RelativeValue\": temp_record[\"value\"][\"nightlyRelative\"]\n",
    "                    }\n",
    "                }\n",
    "            collected_records.append(Point.from_dict(dictionary))\n",
    "        logging.info(\"Recorded Skin Temperature Variation for date \" + start_date_str + \" to \" + end_date_str)\n",
    "    else:\n",
    "        logging.error(\"Recording failed : Skin Temperature Variation for date \" + start_date_str + \" to \" + end_date_str)\n",
    "\n",
    "    spo2_data_list = request_data_from_fitbit('https://api.fitbit.com/1/user/-/spo2/date/' + start_date_str + '/' + end_date_str + '/all.json')\n",
    "    if spo2_data_list != None:\n",
    "        for days in spo2_data_list:\n",
    "            data = days[\"minutes\"]\n",
    "            for record in data: \n",
    "                log_time = datetime.fromisoformat(record[\"minute\"])\n",
    "                utc_time = LOCAL_TIMEZONE.localize(log_time).astimezone(pytz.utc).isoformat()\n",
    "                dictionary = {\n",
    "                        \"measurement\":  \"SPO2_Intraday\",\n",
    "                        \"time\": utc_time,\n",
    "                        \"tags\": {\n",
    "                            \"Device\": DEVICENAME\n",
    "                        },\n",
    "                        \"fields\": {\n",
    "                            \"value\": float(record[\"value\"]),\n",
    "                        }\n",
    "                    }\n",
    "                collected_records.append(Point.from_dict(dictionary))\n",
    "        logging.info(\"Recorded SPO2 intraday for date \" + start_date_str + \" to \" + end_date_str)\n",
    "    else:\n",
    "        logging.error(\"Recording failed : SPO2 intraday for date \" + start_date_str + \" to \" + end_date_str)\n",
    "\n",
    "# Only for sleep data - limit 100 days - 1 query\n",
    "def get_daily_data_limit_100d(start_date_str, end_date_str):\n",
    "\n",
    "    sleep_data = request_data_from_fitbit('https://api.fitbit.com/1.2/user/-/sleep/date/' + start_date_str + '/' + end_date_str + '.json')[\"sleep\"]\n",
    "    if sleep_data != None:\n",
    "        for record in sleep_data:\n",
    "            log_time = datetime.fromisoformat(record[\"startTime\"])\n",
    "            utc_time = LOCAL_TIMEZONE.localize(log_time).astimezone(pytz.utc).isoformat()\n",
    "            try:\n",
    "                minutesLight= record['levels']['summary']['light']['minutes']\n",
    "                minutesREM = record['levels']['summary']['rem']['minutes']\n",
    "                minutesDeep = record['levels']['summary']['deep']['minutes']\n",
    "            except:\n",
    "                minutesLight= record['levels']['summary']['asleep']['minutes']\n",
    "                minutesREM = record['levels']['summary']['restless']['minutes']\n",
    "                minutesDeep = 0\n",
    "\n",
    "            dictionary = {\n",
    "                    \"measurement\":  \"Sleep Summary\",\n",
    "                    \"time\": utc_time,\n",
    "                    \"tags\": {\n",
    "                        \"Device\": DEVICENAME,\n",
    "                        \"isMainSleep\": record[\"isMainSleep\"],\n",
    "                    },\n",
    "                    \"fields\": {\n",
    "                        'efficiency': record[\"efficiency\"],\n",
    "                        'minutesAfterWakeup': record['minutesAfterWakeup'],\n",
    "                        'minutesAsleep': record['minutesAsleep'],\n",
    "                        'minutesToFallAsleep': record['minutesToFallAsleep'],\n",
    "                        'minutesInBed': record['timeInBed'],\n",
    "                        'minutesAwake': record['minutesAwake'],\n",
    "                        'minutesLight': minutesLight,\n",
    "                        'minutesREM': minutesREM,\n",
    "                        'minutesDeep': minutesDeep\n",
    "                    }\n",
    "                }\n",
    "            collected_records.append(Point.from_dict(dictionary))\n",
    "            \n",
    "            sleep_level_mapping = {'wake': 3, 'rem': 2, 'light': 1, 'deep': 0, 'asleep': 1, 'restless': 2, 'awake': 3}\n",
    "            for sleep_stage in record['levels']['data']:\n",
    "                log_time = datetime.fromisoformat(sleep_stage[\"dateTime\"])\n",
    "                utc_time = LOCAL_TIMEZONE.localize(log_time).astimezone(pytz.utc).isoformat()\n",
    "                dictionary = {\n",
    "                        \"measurement\":  \"Sleep Levels\",\n",
    "                        \"time\": utc_time,\n",
    "                        \"tags\": {\n",
    "                            \"Device\": DEVICENAME,\n",
    "                            \"isMainSleep\": record[\"isMainSleep\"],\n",
    "                        },\n",
    "                        \"fields\": {\n",
    "                            'level': sleep_level_mapping[sleep_stage[\"level\"]],\n",
    "                            'duration_seconds': sleep_stage[\"seconds\"]\n",
    "                        }\n",
    "                    }\n",
    "                collected_records.append(Point.from_dict(dictionary))\n",
    "            wake_time = datetime.fromisoformat(record[\"endTime\"])\n",
    "            utc_wake_time = LOCAL_TIMEZONE.localize(wake_time).astimezone(pytz.utc).isoformat()\n",
    "            dictionary = {\n",
    "                        \"measurement\":  \"Sleep Levels\",\n",
    "                        \"time\": utc_wake_time,\n",
    "                        \"tags\": {\n",
    "                            \"Device\": DEVICENAME,\n",
    "                            \"isMainSleep\": record[\"isMainSleep\"],\n",
    "                        },\n",
    "                        \"fields\": {\n",
    "                            'level': sleep_level_mapping['wake'],\n",
    "                            'duration_seconds': None\n",
    "                        }\n",
    "                    }\n",
    "            collected_records.append(Point.from_dict(dictionary))\n",
    "        logging.info(\"Recorded Sleep data for date \" + start_date_str + \" to \" + end_date_str)\n",
    "    else:\n",
    "        logging.error(\"Recording failed : Sleep data for date \" + start_date_str + \" to \" + end_date_str)\n",
    "\n",
    "# Max date range 1 year, records HR zones, Activity minutes and Resting HR - 4 + 3 + 1 + 1 = 9 queries\n",
    "def get_daily_data_limit_365d(start_date_str, end_date_str):\n",
    "    activity_minutes_list = [\"minutesSedentary\", \"minutesLightlyActive\", \"minutesFairlyActive\", \"minutesVeryActive\"]\n",
    "    for activity_type in activity_minutes_list:\n",
    "        activity_minutes_data_list = request_data_from_fitbit('https://api.fitbit.com/1/user/-/activities/tracker/' + activity_type + '/date/' + start_date_str + '/' + end_date_str + '.json')[\"activities-tracker-\"+activity_type]\n",
    "        if activity_minutes_data_list != None:\n",
    "            for data in activity_minutes_data_list:\n",
    "                log_time = datetime.fromisoformat(data[\"dateTime\"] + \"T\" + \"00:00:00\")\n",
    "                utc_time = LOCAL_TIMEZONE.localize(log_time).astimezone(pytz.utc).isoformat()\n",
    "                dictionary = {\n",
    "                        \"measurement\": \"Activity Minutes\",\n",
    "                        \"time\": utc_time,\n",
    "                        \"tags\": {\n",
    "                            \"Device\": DEVICENAME\n",
    "                        },\n",
    "                        \"fields\": {\n",
    "                            activity_type : int(data[\"value\"])\n",
    "                        }\n",
    "                    }\n",
    "                collected_records.append(Point.from_dict(dictionary))\n",
    "            logging.info(\"Recorded \" + activity_type + \"for date \" + start_date_str + \" to \" + end_date_str)\n",
    "        else:\n",
    "            logging.error(\"Recording failed : \" + activity_type + \" for date \" + start_date_str + \" to \" + end_date_str)\n",
    "        \n",
    "\n",
    "    activity_others_list = [\"distance\", \"calories\", \"steps\"]\n",
    "    for activity_type in activity_others_list:\n",
    "        activity_others_data_list = request_data_from_fitbit('https://api.fitbit.com/1/user/-/activities/tracker/' + activity_type + '/date/' + start_date_str + '/' + end_date_str + '.json')[\"activities-tracker-\"+activity_type]\n",
    "        if activity_others_data_list != None:\n",
    "            for data in activity_others_data_list:\n",
    "                log_time = datetime.fromisoformat(data[\"dateTime\"] + \"T\" + \"00:00:00\")\n",
    "                utc_time = LOCAL_TIMEZONE.localize(log_time).astimezone(pytz.utc).isoformat()\n",
    "                activity_name = \"Total Steps\" if activity_type == \"steps\" else activity_type\n",
    "                dictionary = {\n",
    "                        \"measurement\": activity_name,\n",
    "                        \"time\": utc_time,\n",
    "                        \"tags\": {\n",
    "                            \"Device\": DEVICENAME\n",
    "                        },\n",
    "                        \"fields\": {\n",
    "                            \"value\" : float(data[\"value\"])\n",
    "                        }\n",
    "                    }\n",
    "                collected_records.append(Point.from_dict(dictionary))\n",
    "            logging.info(\"Recorded \" + activity_name + \" for date \" + start_date_str + \" to \" + end_date_str)\n",
    "        else:\n",
    "            logging.error(\"Recording failed : \" + activity_name + \" for date \" + start_date_str + \" to \" + end_date_str)\n",
    "        \n",
    "\n",
    "    HR_zones_data_list = request_data_from_fitbit('https://api.fitbit.com/1/user/-/activities/heart/date/' + start_date_str + '/' + end_date_str + '.json')[\"activities-heart\"]\n",
    "    if HR_zones_data_list != None:\n",
    "        for data in HR_zones_data_list:\n",
    "            log_time = datetime.fromisoformat(data[\"dateTime\"] + \"T\" + \"00:00:00\")\n",
    "            utc_time = LOCAL_TIMEZONE.localize(log_time).astimezone(pytz.utc).isoformat()\n",
    "            dictionary = {\n",
    "                    \"measurement\": \"HR zones\",\n",
    "                    \"time\": utc_time,\n",
    "                    \"tags\": {\n",
    "                        \"Device\": DEVICENAME\n",
    "                    },\n",
    "                    \"fields\": {\n",
    "                        \"Normal\" : data[\"value\"][\"heartRateZones\"][0][\"minutes\"],\n",
    "                        \"Fat Burn\" :  data[\"value\"][\"heartRateZones\"][1][\"minutes\"],\n",
    "                        \"Cardio\" :  data[\"value\"][\"heartRateZones\"][2][\"minutes\"],\n",
    "                        \"Peak\" :  data[\"value\"][\"heartRateZones\"][3][\"minutes\"]\n",
    "                    }\n",
    "                }\n",
    "            collected_records.append(Point.from_dict(dictionary))\n",
    "            if \"restingHeartRate\" in data[\"value\"]:\n",
    "                dictionary = {\n",
    "                            \"measurement\":  \"RestingHR\",\n",
    "                            \"time\": utc_time,\n",
    "                            \"tags\": {\n",
    "                                \"Device\": DEVICENAME\n",
    "                            },\n",
    "                            \"fields\": {\n",
    "                                \"value\": data[\"value\"][\"restingHeartRate\"]\n",
    "                            }\n",
    "                        }\n",
    "                collected_records.append(Point.from_dict(dictionary))\n",
    "        logging.info(\"Recorded RHR and HR zones for date \" + start_date_str + \" to \" + end_date_str)\n",
    "    else:\n",
    "        logging.error(\"Recording failed : RHR and HR zones for date \" + start_date_str + \" to \" + end_date_str)\n",
    "\n",
    "# records SPO2 single days for the whole given period - 1 query\n",
    "def get_daily_data_limit_none(start_date_str, end_date_str):\n",
    "    data_list = request_data_from_fitbit('https://api.fitbit.com/1/user/-/spo2/date/' + start_date_str + '/' + end_date_str + '.json')\n",
    "    if data_list != None:\n",
    "        for data in data_list:\n",
    "            log_time = datetime.fromisoformat(data[\"dateTime\"] + \"T\" + \"00:00:00\")\n",
    "            utc_time = LOCAL_TIMEZONE.localize(log_time).astimezone(pytz.utc).isoformat()\n",
    "            dictionary = {\n",
    "                    \"measurement\":  \"SPO2\",\n",
    "                    \"time\": utc_time,\n",
    "                    \"tags\": {\n",
    "                        \"Device\": DEVICENAME\n",
    "                    },\n",
    "                    \"fields\": {\n",
    "                        \"avg\": data[\"value\"][\"avg\"],\n",
    "                        \"max\": data[\"value\"][\"max\"],\n",
    "                        \"min\": data[\"value\"][\"min\"]\n",
    "                    }\n",
    "                }\n",
    "            collected_records.append(Point.from_dict(dictionary))\n",
    "        logging.info(\"Recorded Avg SPO2 for date \" + start_date_str + \" to \" + end_date_str)\n",
    "    else:\n",
    "        logging.error(\"Recording failed : Avg SPO2 for date \" + start_date_str + \" to \" + end_date_str)\n",
    "\n",
    "# Fetches latest activities from record ( upto last 100 )\n",
    "def fetch_latest_activities(end_date_str):\n",
    "    recent_activities_data = request_data_from_fitbit('https://api.fitbit.com/1/user/-/activities/list.json', params={'beforeDate': end_date_str, 'sort':'desc', 'limit':50, 'offset':0})\n",
    "    if recent_activities_data != None:\n",
    "        for activity in recent_activities_data['activities']:\n",
    "            fields = {}\n",
    "            if 'activeDuration' in activity:\n",
    "                fields['ActiveDuration'] = int(activity['activeDuration'])\n",
    "            if 'averageHeartRate' in activity:\n",
    "                fields['AverageHeartRate'] = int(activity['averageHeartRate'])\n",
    "            if 'calories' in activity:\n",
    "                fields['calories'] = int(activity['calories'])\n",
    "            if 'duration' in activity:\n",
    "                fields['duration'] = int(activity['duration'])\n",
    "            if 'distance' in activity:\n",
    "                fields['distance'] = float(activity['distance'])\n",
    "            if 'steps' in activity:\n",
    "                fields['steps'] = int(activity['steps'])\n",
    "            starttime = datetime.fromisoformat(activity['startTime'].strip(\"Z\"))\n",
    "            utc_time = starttime.astimezone(pytz.utc).isoformat()\n",
    "            dictionary = {\n",
    "                \"measurement\": \"Activity Records\",\n",
    "                \"time\": utc_time,\n",
    "                \"tags\": {\n",
    "                    \"ActivityName\": activity['activityName']\n",
    "                },\n",
    "                \"fields\": fields\n",
    "            }\n",
    "            collected_records.append(Point.from_dict(dictionary))\n",
    "        logging.info(\"Fetched 50 recent activities before date \" + end_date_str)\n",
    "    else:\n",
    "        logging.error(\"Fetching 50 recent activities failed : before date \" + end_date_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Timezone from profile data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOCAL_TIMEZONE == \"Automatic\":\n",
    "    LOCAL_TIMEZONE = pytz.timezone(request_data_from_fitbit(\"https://api.fitbit.com/1/user/-/profile.json\")[\"user\"][\"timezone\"])\n",
    "else:\n",
    "    LOCAL_TIMEZONE = pytz.timezone(LOCAL_TIMEZONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call the functions one time as a startup update OR do switch to bulk update mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(collected_records))\n",
    "collected_records = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "if AUTO_DATE_RANGE:\n",
    "    date_list = [(start_date + timedelta(days=i)).strftime(\"%Y-%m-%d\") for i in range((end_date - start_date).days + 1)]\n",
    "    if len(date_list) > 3:\n",
    "        logging.warn(\"Auto schedule update is not meant for more than 3 days at a time, please consider lowering the auto_update_date_range variable to aviod rate limit hit!\")\n",
    "    for date_str in date_list:\n",
    "        get_intraday_data_limit_1d(date_str, [('heart','HeartRate_Intraday','1sec'),('steps','Steps_Intraday','1min')]) # 2 queries x number of dates ( default 2)\n",
    "    get_daily_data_limit_30d(start_date_str, end_date_str) # 3 queries\n",
    "    get_daily_data_limit_100d(start_date_str, end_date_str) # 1 query\n",
    "    get_daily_data_limit_365d(start_date_str, end_date_str) # 8 queries\n",
    "    get_daily_data_limit_none(start_date_str, end_date_str) # 1 query\n",
    "    get_battery_level() # 1 query\n",
    "    fetch_latest_activities(end_date_str) # 1 query\n",
    "    write_points_to_influxdb(collected_records)\n",
    "    collected_records = []\n",
    "else:\n",
    "    # Do Bulk update----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    schedule.every(1).hours.do(lambda : Get_New_Access_Token(client_id,client_secret)) # Auto-refresh tokens every 1 hour\n",
    "    \n",
    "    date_list = [(start_date + timedelta(days=i)).strftime(\"%Y-%m-%d\") for i in range((end_date - start_date).days + 1)]\n",
    "\n",
    "    def yield_dates_with_gap(date_list, gap):\n",
    "        start_index = -1*gap\n",
    "        while start_index < len(date_list)-1:\n",
    "            start_index  = start_index + gap\n",
    "            end_index = start_index+gap\n",
    "            if end_index > len(date_list) - 1:\n",
    "                end_index = len(date_list) - 1\n",
    "            if start_index > len(date_list) - 1:\n",
    "                break\n",
    "            yield (date_list[start_index],date_list[end_index])\n",
    "\n",
    "    def do_bulk_update(funcname, start_date, end_date):\n",
    "        global collected_records\n",
    "        funcname(start_date, end_date)\n",
    "        schedule.run_pending()\n",
    "        write_points_to_influxdb(collected_records)\n",
    "        collected_records = []\n",
    "\n",
    "    fetch_latest_activities(date_list[-1])\n",
    "    write_points_to_influxdb(collected_records)\n",
    "    do_bulk_update(get_daily_data_limit_none, date_list[0], date_list[-1])\n",
    "    for date_range in yield_dates_with_gap(date_list, 360):\n",
    "        do_bulk_update(get_daily_data_limit_365d, date_range[0], date_range[1])\n",
    "    for date_range in yield_dates_with_gap(date_list, 98):\n",
    "        do_bulk_update(get_daily_data_limit_100d, date_range[0], date_range[1])\n",
    "    for date_range in yield_dates_with_gap(date_list, 28):\n",
    "        do_bulk_update(get_daily_data_limit_30d, date_range[0], date_range[1])\n",
    "    for single_day in date_list:\n",
    "        do_bulk_update(get_intraday_data_limit_1d, single_day, [('heart','HeartRate_Intraday','1sec'),('steps','Steps_Intraday','1min')])\n",
    "\n",
    "    logging.info(\"Success : Bulk update complete for \" + start_date_str + \" to \" + end_date_str)\n",
    "    print(\"Bulk update complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schedule functions at specific intervals (Ongoing continuous update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/dangy/git/public-fitbit-projects/Fitbit_Fetch.ipynb Cell 23\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/dangy/git/public-fitbit-projects/Fitbit_Fetch.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     write_points_to_influxdb(collected_records)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/dangy/git/public-fitbit-projects/Fitbit_Fetch.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     collected_records \u001b[39m=\u001b[39m []\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/dangy/git/public-fitbit-projects/Fitbit_Fetch.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m time\u001b[39m.\u001b[39;49msleep(\u001b[39m30\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/dangy/git/public-fitbit-projects/Fitbit_Fetch.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m update_working_dates()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Ongoing continuous update of data\n",
    "if SCHEDULE_AUTO_UPDATE:\n",
    "    \n",
    "    schedule.every(1).hours.do(lambda : Get_New_Access_Token(client_id,client_secret)) # Auto-refresh tokens every 1 hour\n",
    "    schedule.every(3).minutes.do( lambda : get_intraday_data_limit_1d(end_date_str, [('heart','HeartRate_Intraday','1sec'),('steps','Steps_Intraday','1min')] )) # Auto-refresh detailed HR and steps\n",
    "    schedule.every(20).minutes.do(get_battery_level) # Auto-refresh battery level\n",
    "    schedule.every(3).hours.do(lambda : get_daily_data_limit_30d(start_date_str, end_date_str))\n",
    "    schedule.every(4).hours.do(lambda : get_daily_data_limit_100d(start_date_str, end_date_str))\n",
    "    schedule.every(6).hours.do( lambda : get_daily_data_limit_365d(start_date_str, end_date_str))\n",
    "    schedule.every(6).hours.do(lambda : get_daily_data_limit_none(start_date_str, end_date_str))\n",
    "    schedule.every(1).hours.do( lambda : fetch_latest_activities(end_date_str))\n",
    "\n",
    "    while True:\n",
    "        schedule.run_pending()\n",
    "        if len(collected_records) != 0:\n",
    "            write_points_to_influxdb(collected_records)\n",
    "            collected_records = []\n",
    "        time.sleep(30)\n",
    "        update_working_dates()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-conda-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
